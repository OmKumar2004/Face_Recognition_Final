{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Recognition and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries to load and test the models\n",
    "from sklearn.metrics import accuracy_score,classification_report,ConfusionMatrixDisplay\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Feature Extraction we found out that HoG, meaning Histogram of Oriented Gradients, we notice that we get around 70,000 features which is quite bulky for models to handle, so we applied PCA on the HoG features alone and extracted 985 features out of those 70,000 which covers 0.95 variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the dataset\n",
    "Dataset was initally broken into two parts train and test. However, we realised that we had to drop many classes due to lack of sufficient images to train. Thus, we concatenated these two datasets together and had carried out training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset part 01 contains 10,586 records with 3248 features\n",
    "# Dataset part 02 contains 2647 records with 3248 features\n",
    "dataset_part_01 = pd.read_csv(\"\",header = None)\n",
    "dataset_part_02 = pd.read_csv(\"\",header = None)\n",
    "\n",
    "dataset = pd.concat([dataset_part_01,dataset_part_02],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head())\n",
    "\n",
    "print(dataset.info())\n",
    "\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may notice that that features are not in the same range, so, we normalized the data as Zero Mean and One Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We load the models trained and tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face_Recognition:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.images = None\n",
    "        self.__models = []\n",
    "        self.__model_weights = {}\n",
    "    \n",
    "    def __loadModels (self):\n",
    "        # We are loading the hyper parameter tuned models that were saved using joblib\n",
    "\n",
    "        model_Decision_Tree = joblib.load(\"*\")\n",
    "        self.__models.append(model_Decision_Tree)\n",
    "        self.__model_weights[model_Decision_Tree] = 10\n",
    "\n",
    "        model_Random_Forest = joblib.load(\"*\")\n",
    "        self.__models.append(model_Random_Forest)\n",
    "        self.__model_weights[model_Random_Forest] = 10\n",
    "\n",
    "\n",
    "        model_SVM = joblib.load(\"*\")\n",
    "        self.__models.append(model_SVM)\n",
    "        self.__model_weights[model_SVM] = 10\n",
    "\n",
    "        model_ANN = joblib.load(\"*\")\n",
    "        self.__models.append(model_ANN)\n",
    "        self.__model_weights[model_ANN] = 10\n",
    "\n",
    "        model_kNN = joblib.load(\"*\")\n",
    "        self.__models.append(model_kNN)\n",
    "        self.__model_weights[model_kNN] = 10\n",
    "\n",
    "\n",
    "        model_Random_Landscape = joblib.load(\"*\")\n",
    "        self.__models.append(model_Random_Landscape)\n",
    "        self.__model_weights[model_Random_Landscape] = 10\n",
    "\n",
    "        model_Naive_Bayes = joblib.load(\"*\")\n",
    "        self.__models.append(model_Naive_Bayes)\n",
    "        self.__model_weights[model_Naive_Bayes] = 10\n",
    "\n",
    "    \n",
    "    def __printModel_Parameters (self):\n",
    "        # Printing the models with their Hyper Parameters\n",
    "\n",
    "        print(\"Decision Tree \",self.__models[0])\n",
    "\n",
    "        print(\"kNN \",self.__models[4])\n",
    "\n",
    "        print(\"Artificial Neural Network \",self.__models[3])\n",
    "\n",
    "        print(\"Random Forest \",self.__models[1])\n",
    "\n",
    "        print(\"Random Landscape\",self.__models[5])\n",
    "\n",
    "        print(\"Support Vector Machine \",self.__models[2])\n",
    "\n",
    "        print(\"Naive Bayes \",self.__models[6])\n",
    "    \n",
    "\n",
    "    def predict(self,image):\n",
    "\n",
    "        ## Call the feature extraction function and store it in the variable X ##\n",
    "\n",
    "        ## Set up the model weights ##\n",
    "\n",
    "        ## Predictions are stored here\n",
    "        y_predictions = []\n",
    "\n",
    "        # Receive prediction of each model\n",
    "        for model in self.__models:\n",
    "\n",
    "            model_prediction = model.predict(X)\n",
    "\n",
    "            # Add the prediction of model the weight number of times to get better performance\n",
    "\n",
    "            for iter in range(self.__model_weights[model]):\n",
    "                y_predictions.append(model_prediction)\n",
    "        \n",
    "\n",
    "        # Get the count that has max frequency and return that prediction\n",
    "        max_count = 0\n",
    "        predicted_class = ''\n",
    "\n",
    "        for element in y_predictions:\n",
    "\n",
    "            count = y_predictions.count(element)\n",
    "            if y_predictions.count(element) > max_count:\n",
    "                max_count = count\n",
    "                predicted_class = element\n",
    "            \n",
    "        return predicted_class\n",
    "    \n",
    "    def get_similarity (self,image1,image2):\n",
    "\n",
    "        ### Write the code here ###\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceRecognition = Face_Recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
